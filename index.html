<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@100;200;300;400;500;600;700;800;900&display=swap" rel="stylesheet">

    <title>Home | Christopher Housholder</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">


    <!-- Additional CSS Files -->
    <link rel="stylesheet" href="assets/css/fontawesome.css">
    <link rel="stylesheet" href="assets/css/style.css">
    <link rel="stylesheet" href="assets/css/owl.css">
    <link rel="stylesheet" href="assets/css/animate.css">
    <link rel="stylesheet"href="https://unpkg.com/swiper@7/swiper-bundle.min.css"/>

  </head>

<body>

  <!-- ***** Preloader Start ***** -->
  <div id="js-preloader" class="js-preloader">
    <div class="preloader-inner">
      <span class="dot"></span>
      <div class="dots">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  </div>
  <!-- ***** Preloader End ***** -->

  <!-- ***** Header Area Start ***** -->
  <header class="header-area header-sticky">
    <div class="container">
        <div class="row">
            <div class="col-12">
                <nav class="main-nav">
                    <!-- ***** Logo Start ***** -->
                    <a href="index.html" class="logo">
                        <img src="assets/images/logo.png" alt="">
                    </a>
                    <!-- ***** Logo End ***** -->
                    <!-- ***** Menu Start ***** -->
                    <ul class="nav">
                        <li><a href="index.html" class="active">Home</a></li>
                        <li><a href="projects.html">Browse</a></li>
                        <li><a href="resume.html">Resume</a></li>
                        <li><a href="profile.html">Profile <img src="assets/images/profile-header.png" alt=""></a></li>
                    </ul>
                    <a class='menu-trigger'>
                        <span>Menu</span>
                    </a>
                    <!-- ***** Menu End ***** -->
                </nav>
            </div>
        </div>
    </div>
  </header>
  <!-- ***** Header Area End ***** -->

  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <div class="page-content">

          <!-- ***** Banner Start ***** -->
          <div class="main-banner">
            <div class="row">
              <div class="col-lg-7">
                <div class="header-text">
                  <h6>Researcher | Pure Mathematician | Data Scientist</h6>
                  <h4><em>Browse</em> My Projects Here</h4>
                  <div class="main-button">
                    <a href="projects.html">View My Work</a>
                  </div>
                  <div class="main-button2">
                    <a href="profile.html#contact-form">Contact Me</a>
                  </div>
                </div>
              </div>
            </div>
          </div>
          <!-- ***** Banner End ***** -->

          <!-- ***** About Me Start ***** -->
          <section class="about-me" style="margin-top: 40px;">
            <div class="row">
              <div class="col-lg-12">
                <div style="
                      background-color: #1f2122;
                      padding: 30px;
                      border-radius: 23px;
                      box-shadow: 0 0 15px rgba(0,0,0,0.2);
                      margin-top: 20px;">
                  <div class="heading-section">
                    <h4>Who am I?</h4>
                  </div>
                  <p style="color: #fff; font-size: 16px; line-height: 1.8; margin: 0;">
                    I’m a driven researcher and aspiring pure mathematician with a focus on number theory, analysis, and spectral theory. I’ve published and presented research across multiple domains, including theoretical AI and combinatorics. My long-term goal is to pursue a Ph.D. at a top-tier institution and become a professor dedicated to advancing foundational mathematics.
                  </p>
                </div>
              </div>
            </div>
          </section>
          <!-- ***** About Me End ***** -->



          <!-- ***** Most Popular Start ***** -->
          <div class="most-popular">
            <div class="row">
              <div class="col-lg-12">
                <div class="heading-section">
                  <h4 style="color: #0dcaf0;">Research</h4>
                </div>
                <div class="row">
                  <div class="col-lg-6 col-sm-6">
                    <div class="item popular-rect">
                      <img src="assets/images/Portfolio1.png" alt="">
                      <h4>Architectural redundancies in the Dozerformer attention model on time-series predictions.<br><span>We study the structure of the Dozerformer attention model in the context of continuous time series prediction, with the goal of identifying architectural redundancies. In particular, we evaluate whether either the encoder or decoder module can be removed without significantly impacting performance. To this end, we conduct a set of ablation experiments in which each component is independently removed and the resulting model is trained and evaluated across a range of continuous time series tasks. Our results show that the encoder plays a critical role in learning and maintaining temporal structure, with its removal leading to sharp declines in predictive accuracy. In contrast, removing the decoder results in little to no performance loss in most cases, suggesting that it may be partially redundant in this setting. These findings highlight the asymmetry of importance between encoder and decoder in time series applications and suggest that attention-based models for continuous signals can be simplified without compromising effectiveness. This contributes to ongoing efforts to streamline transformer architectures and improve efficiency in real-world temporal modeling tasks.</span></h4>
                      <div class="bottom-bar">
                        <p class="left-note">CSTA Annual Conference (2025)</p>
                        <ul id="stars-gold-1">
                          <li><i class="fa fa-star"></i> 1st</li>
                        </ul>
                      </div>
                    </div>
                  </div>
                  <div class="col-lg-6 col-sm-6">
                    <div class="item popular-rect">
                      <img src="assets/images/Portfolio2.png" alt="">
                      <h4>VC-Dimensions of subsets of the Hamming graph.<br><span>We study the structure of the Dozerformer attention model in the context of continuous time series prediction, with the goal of identifying architectural redundancies. In particular, we evaluate whether either the encoder or decoder module can be removed without significantly impacting performance. To this end, we conduct a set of ablation experiments in which each component is independently removed and the resulting model is trained and evaluated across a range of continuous time series tasks. Our results show that the encoder plays a critical role in learning and maintaining temporal structure, with its removal leading to sharp declines in predictive accuracy. In contrast, removing the decoder results in little to no performance loss in most cases, suggesting that it may be partially redundant in this setting. These findings highlight the asymmetry of importance between encoder and decoder in time series applications and suggest that attention-based models for continuous signals can be simplified without compromising effectiveness. This contributes to ongoing efforts to streamline transformer architectures and improve efficiency in real-world temporal modeling tasks.</span></h4>
                      <div class="bottom-bar">
                        <p class="left-note">Combinatorial and Additive Number Theory (CANT 2025)</p>
                        <ul id="stars-gold-1">
                          <li><i class="fa fa-star"></i> 1st</li>
                        </ul>
                      </div>
                    </div>
                  </div>
                  <div class="col-lg-6 col-sm-6">
                    <div class="item popular-rect">
                      <img src="assets/images/Portfolio3.png" alt="">
                      <h4>Explicit repeated dot product tree constructions<br><span>We study dot product trees, which are configurations of points in Euclidean space where edge weights are defined by the dot product of point pairs. For any fixed tree with <i>k</i> edges, we construct a finite point set in ℝ<sup>d</sup> that contains at least <i>C</i><i>n</i><sup>(k+1)/2</sup> isomorphic embeddings of the tree with a specified sequence of dot product values, for some constant <i>C</i> &gt; 0. Our approach uses a bipartite coloring of the tree and places points along structured lines to generate repeated patterns. This construction improves known lower bounds and narrows the gap with existing upper bounds in related extremal problems. Motivated by Erdős’s unit distance problem, our work highlights the frequency of repeated dot product configurations and provides new tools for analyzing structured subsets in discrete geometry. These results have further implications in additive combinatorics and theoretical computer science.</span></h4>
                      <div class="bottom-bar">
                        <p class="left-note">Combinatorial and Additive Number Theory (CANT 2024)</p>
                        <ul id="stars-silver-1">
                          <li><i class="fa fa-star"></i> 2nd</li>
                        </ul>
                      </div>
                    </div>
                  </div>
                  <div class="col-lg-6 col-sm-6">
                    <div class="item popular-rect">
                      <img src="assets/images/Portfolio4.png" alt="">
                      <h4>Convergence of classical and nonlocal curvature.<br><span>We study the recently introduced notion of nonlocal mean curvature, defined via an integral operator involving a convolution kernel. Specifically, we consider curvature of a set Ω ⊂ ℝ² defined by an integrable, radially symmetric, nonnegative, and nonincreasing kernel J, and examine the behavior of the resulting nonlocal curvature functional. Building on previous work, we extend the definition to higher dimensions and investigate its convergence properties in the classical limit. Our main result shows that for sets with C²-boundary, nonlocal mean curvature converges to the classical mean curvature as the support of the kernel shrinks to zero. This confirms that integrable kernels can faithfully recover local geometric information and provides a concrete bridge between nonlocal and classical curvature models.</span></h4>
                      <div class="bottom-bar">
                        <p class="left-note">CNAS Undergraduate Research Symposium</p>
                        <ul id="stars-silver-1">
                          <li><i class="fa fa-star"></i> 2nd</li>
                        </ul>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
          <!-- ***** Most Popular End ***** -->

  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-12">
          <p>Copyright © 2025 <a href="#">Christopher Housholder</a>. All rights reserved.</p>
        </div>
      </div>
    </div>
  </footer>


  <!-- Scripts -->
  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

  <script src="assets/js/isotope.min.js"></script>
  <script src="assets/js/owl-carousel.js"></script>
  <script src="assets/js/tabs.js"></script>
  <script src="assets/js/popup.js"></script>
  <script src="assets/js/custom.js"></script>


  </body>

</html>
